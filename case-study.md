# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решила исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумала использовать такие метрики:
- замер времени выполнения скрипта при помощи вызова Benchmark.realtime;
- замер использования выделяемой памяти для процесса.

Для замеров берем часть обрабатываемого файла, размером 1 Мб.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроила эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений.

Вот как я построила `feedback_loop`:
- запускаем скрипт с подключением инструмента профилирования;
- на основе результатов вносим изменения в код;
- проверяем основную метрику, что внесенное изменение не приводит к регрессии.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовалась следующими инструментами: stackprof, stackprof, ruby-prof, qcachegrind.

Вот какие проблемы удалось найти и решить:

### Находка №1
При помощи библиотеки stackprof определили, что в методе work в цикле обработки строк из файла при сборе массива sessions происходит большое кол-во аллокаций, так как для добавления нового элемента в массив каждый раз создавался новый массив из одного элемента.
Небольшое изменение кода позволило снизить потребляемую процессом память с ~118 Мб до ~83 Мб.
На скорость работы скрипта это практически не повлияло (время уменьшилось в размерах измерительной погрешности).

На основе данной оптимизации в двух других местах скрипта были заменены подобные языковые конструкции, что позволило сократить используемую память ~56Мб.

### Находка №2
При запуске memory_profiler становится очевидным, что происходит постоянное создание одинаковых строк, которые являются ключами в хэшах.
Заменим строки на символы. Попутно, убрав из хэшей пользователя и сессия не используемые данные.
Эта оптимизация позволила сократить время работы скрипта до 11.21 сек.

### Находка №3
При следующем запуске memory_profiler'а обратила внимание, что самое большое выделение памяти идёт на строка выбора сессий для пользователя из общего массива.
Решение этой проблемы - изначально хранить сессии в хэше с ключами - id пользователей.
Изменение в хранении сессий вынудило так же перенести сбор наименований браузеров в основной итератор по строкам файла.
Резальтатом данного рефакторинга стало стремительное уменьшение времени работы скрипта.
Теперь файл в среднем обрабатывается ~0.42 сек при потреблении ~39 Мб памяти.

### Находка №4
Запустила ruby-prof+qcachegrind. Стало очевидно, что наибольшее число аллокаций происходит при переборе массивов при вызове метода collect_stats_from_users. Проблема в том, что метод вызывается 7 раз для сбора информации, когда можно всё собрать за один проход. Тем более, что выше уже есть перебор всех пользователей для сбора user_objects. Гипотиза - переписать сбор данных в один проход по массиву пользователей.
Думала, что данная оптимизация принесёт больше пользы.
Теперь файл в среднем обрабатывается ~0.41 сек при потреблении ~34 Мб памяти.
То есть улучшения в рамках погрешности.
Возможно как-то связано с использованием патченой версии руби для этой итерации.

### Находка №5
Снова запустила stackprof. Наибольшее кол-во аллокаций - строка с манипуляциями с датой.
Перенесла конвертацию даты в необходимый формат в метод парсинга данных.
Сам метод сбора данных по датам в отчёт стал требовать меньшего кол-ва аллокаций, но улучшения показателей скорости работы и занимаемой памяти в рамках погрешности, что странно.

### Находка №6
Заменила чтение файла на построчное.
Улучшения показателей скорости работы и занимаемой памяти в рамках погрешности. Возможно, это из-за малого размера файла с данными. И на полном размере разница была бы ощутима.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы при обработке файла объёмом 1Мб с 14.97 сек, до обработки за ~0.41 сек.
Полный файл с данными обрабатывается за 78.09 сек, при этом выполнение скрипта потребляет 2894 Мб памяти.

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы сделано хммм... ничего?
