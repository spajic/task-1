# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я решил использовать такую метрику: сколько раз в секунду программа успевает обработать файл размером 10000 строк (IPS)

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 20 секунд

Вот как я построил `feedback_loop`: создал скрипт `optimization_loop.rb` который автоматически запускал тест для проверки что ничего не сломано, и сразу же измерял целевую метрику.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался Memory Profiler, StackProf, RubyProf, Vallgrind Massif.

Вот какие проблемы удалось найти и решить

### Находка №1
Неявный вложенный цикл в блоке select, исправил на предварительную сортировку сессий сразу после парсинга

### Находка №2
Неефективный сбор уникальных браузеров, переделал на множество

### Находка №3
Медленная генерация JSON, решение: использовал Oj

На данном этапе файл уже можно бло обработать за разумное время меншье 30 секунд и далее упор был сделан на уменьшение размера потребляемой памяти при обработке большого файла.

### Находка №4
Файл загружается и парсится в память целиком. Решение: файл обрабатывается построчно, новые данные пошагово агрегируются с предыдущими

### Находка №4
Программа хранить в памяти большие агрегаты (список всех браузеров и дат всех сессий). Решение: так как сессии в файле идут группированные по пользователю, то запись результатов в файл ведётся поточно для каждого пользователя, и в один момент времени в памяти хранится агрегат только для текущего пользвателя, также больше не хранится список пользователей

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы (IPS) с 0.482, 25.237

Также была замерена асимтотика времени выполнения программы, и после оптимизации она стала почти линейной https://docs.google.com/spreadsheets/d/1g9OB3SIYXMOdno_0iH3xMKYZpoD3YuL9I0bo0YdyUDQ/edit?usp=sharing

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был добавлен performance test
