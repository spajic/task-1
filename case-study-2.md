# Case-study оптимизация. Часть 2.

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: объем затрачиваемой памяти и время выполнения

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Асимптотика
До оптимизации:
Calculating -------------------------------------
         File: 512Kb      4.817  (± 5.3%) i/s -     24.000  in   5.032638s
           File: 1Mb      2.527  (± 6.3%) i/s -     13.000  in   5.183298s
           File: 2Mb      1.203  (±15.4%) i/s -      6.000  in   5.132240s
           File: 4Mb      0.586  (±19.5%) i/s -      3.000  in   5.269748s
           File: 8Mb      0.287  (±14.0%) i/s -      2.000  in   7.100129s
                   with 99.0% confidence

Comparison:
         File: 512Kb:        4.8 i/s
           File: 1Mb:        2.5 i/s - 1.91x  (± 0.15) slower
           File: 2Mb:        1.2 i/s - 4.00x  (± 0.68) slower
           File: 4Mb:        0.6 i/s - 8.18x  (± 1.96) slower
           File: 8Mb:        0.3 i/s - 16.77x  (± 3.11) slower

Исходя из данных показателей мы видим, что увеличение объема обрабатываемых данных в 2 раза ведет к росту времени работы в 2 раза

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений в среднем за 10 секунд

Вот как я построил `feedback_loop`:
1. Изначально выбрал маленький размер исходных данных (около 1Мб) позволяющий скрипту успешно отработать без оптимизаций
2. Поиск базовой метрики (время и память)
3. Дописал тест на регрессию по времени и памяти
4. Профилирование и поиск "точек роста"
5. Внесение изменений в код
6. Повторное тестирование и сбор новых метрик
7. Увеличение объема данных

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался

Гемы:
* ruby-prof
* memory_profiler
* get_process_mem
* benchmark-ips
Stdlib:
* benchmark
C
* valgrind (tool - massif)

Вот какие проблемы удалось найти и решить

### Находка №1

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
В среднем файл обрабатывается за 29 - 30 секунд при средних затратах по памяти 850 - 860Мб

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавлены дополнительные тесты для защиты от регрессий по памяти и времени выполнения
