# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
* Сначала я написал свой "велосипед", который запускал код несколько раз, измерял время выполнения через `Benchmark.realtime` и считал среднее.
* Но потом я подсмотрел в разборе из второй лекции инструмент `Benchmark.ips` и он мне понравился

В итоге метрика это число итераций в секунду

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации. Но этот тест был на минитесте и я решил переписать его на более привычный мне `RSpec`. В тесте я зафиксиовал что программа возвращает теже результаты и что программа не стала работать медленнее.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 55 секунд.

Вот как я построил `feedback_loop`:
* Все эталонные файлы лежат папке `spec/fixtures`
* Файлы, которые раньше генерировал минитест теперь называются `data.txt` и `reference_result.json`
* Тест smoke_test_spec.rb проверяет что:
** вывод программы все еще соответствует эталонным файлам
** выполнение программы не стало медленее чем было в начале работы (3.5 секунд в моем случае)
* Скрипт `calc_times.rb` считает метрику запуская `Benchmark.ips` по набору файлов от 100 строк до 5к строк
* Запуск всего Feedback-Loop производится одним скриптом `test-it.sh`

## Фиксируем начальное состояние
До начала всяких оптимизаций наша метрика была вот такой
```
Calculating -------------------------------------
                0.1k    495.917  (±11.1%) i/s -      2.433k in   4.998076s
                0.2k    222.219  (± 9.9%) i/s -      1.096k in   5.001490s
                0.3k    136.152  (± 9.5%) i/s -    673.000  in   5.000267s
                0.4k     93.109  (± 9.7%) i/s -    461.000  in   5.010672s
                0.5k     68.628  (± 7.3%) i/s -    341.000  in   5.010336s
                 01k     25.422  (± 3.9%) i/s -    127.000  in   5.014775s
                 02k      8.279  (± 0.0%) i/s -     42.000  in   5.082306s
                 03k      4.093  (± 0.0%) i/s -     21.000  in   5.134921s
                 04k      2.464  (± 0.0%) i/s -     13.000  in   5.281077s
                 05k      1.658  (± 0.0%) i/s -      9.000  in   5.428681s

Comparison:
                0.1k:      495.9 i/s
                0.2k:      222.2 i/s - 2.23x  slower
                0.3k:      136.2 i/s - 3.64x  slower
                0.4k:       93.1 i/s - 5.33x  slower
                0.5k:       68.6 i/s - 7.23x  slower
                 01k:       25.4 i/s - 19.51x  slower
                 02k:        8.3 i/s - 59.90x  slower
                 03k:        4.1 i/s - 121.17x  slower
                 04k:        2.5 i/s - 201.25x  slower
                 05k:        1.7 i/s - 299.06x  slower
```
То есть при увеличении объема в 50 раз метрика ухужшалась почти в 300 раз

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я проверял следующие гипотезы

### Приложение тратит время на сборку мусора
Сделал `GC.disable` - гипотеза не потдтвердилась. Сборщик мусора не повлиял на производительность

### Найти самые медленные части системы



Вот какие проблемы удалось найти и решить

### Находка №1
Используя `RubyProf` нахожу что выполняется медленнее всего. 94.11% времени занимает `Array#each`, что логично, потому что мы все время что-то делаем с масивами. Из времени внутри `Array#each` 42.55% занимает `Array#select`, который в коде всего один - значит его и будем оптимизировать в первую очередь
```
users.each do |user|
  ...
  user_sessions = sessions.select { |session| session['user_id'] == user['id'] }  
```
Тут мы имеем дело со вложенным циклом. Решение: вынести расчет пользовательских сессий в предварительный расчет.

После оптимизации этого участка получаем работающие тесты и вот такой отчет
```
Calculating -------------------------------------
                0.1k    532.154  (±11.8%) i/s -      2.602k in   4.996905s
                0.2k    259.963  (±10.4%) i/s -      1.280k in   4.999759s
                0.3k    167.843  (±10.7%) i/s -    825.000  in   5.000048s
                0.4k    122.780  (± 9.8%) i/s -    603.000  in   5.005309s
                0.5k     99.104  (± 9.1%) i/s -    491.000  in   5.005207s
                 01k     45.688  (± 8.8%) i/s -    226.000  in   5.002644s
                 02k     22.265  (± 4.5%) i/s -    111.000  in   5.006596s
                 03k     14.118  (± 7.1%) i/s -     71.000  in   5.043650s
                 04k      9.584  (± 0.0%) i/s -     48.000  in   5.022222s
                 05k      7.319  (± 0.0%) i/s -     37.000  in   5.063886s

Comparison:
                0.1k:      532.2 i/s
                0.2k:      260.0 i/s - 2.05x  slower
                0.3k:      167.8 i/s - 3.17x  slower
                0.4k:      122.8 i/s - 4.33x  slower
                0.5k:       99.1 i/s - 5.37x  slower
                 01k:       45.7 i/s - 11.65x  slower
                 02k:       22.3 i/s - 23.90x  slower
                 03k:       14.1 i/s - 37.69x  slower
                 04k:        9.6 i/s - 55.53x  slower
                 05k:        7.3 i/s - 72.70x  slower
```
То есть при увеличении объема в 50 раз метрика ухужшалась почти в 73 раз, против 300, до оптимизации

Время выполнения прогона для файла 10к строк уменьшилось с 3.5 секунд до 0.3557 - поменяем это в тестах

### Находка №2
Следующий кандидат на улучшение это метод collect_stats_from_users занимающий 43.53% относительного времени.
Убираем повторные вызовы и двойной расчет одного и того же. После оптимизации получаем некоторый прирост
```
Calculating -------------------------------------
                0.1k    587.188  (±12.6%) i/s -      2.850k in   4.997908s
                0.2k    276.606  (±16.6%) i/s -      1.311k in   5.000084s
                0.3k    175.600  (±15.9%) i/s -    846.000  in   5.002895s
                0.4k    135.578  (±14.8%) i/s -    638.000  in   5.000373s
                0.5k    103.587  (±17.4%) i/s -    487.000  in   5.005707s
                 01k     49.748  (±16.1%) i/s -    234.000  in   5.000097s
                 02k     24.272  (± 8.2%) i/s -    121.000  in   5.017043s
                 03k     15.061  (± 6.6%) i/s -     75.000  in   5.016839s
                 04k     10.365  (± 9.6%) i/s -     52.000  in   5.044044s
                 05k      8.533  (± 0.0%) i/s -     43.000  in   5.048907s

Comparison:
                0.1k:      587.2 i/s
                0.2k:      276.6 i/s - 2.12x  slower
                0.3k:      175.6 i/s - 3.34x  slower
                0.4k:      135.6 i/s - 4.33x  slower
                0.5k:      103.6 i/s - 5.67x  slower
                 01k:       49.7 i/s - 11.80x  slower
                 02k:       24.3 i/s - 24.19x  slower
                 03k:       15.1 i/s - 38.99x  slower
                 04k:       10.4 i/s - 56.65x  slower
                 05k:        8.5 i/s - 68.81x  slower
```
Время выполнения прогона для файла 10к строк уменьшилось с 0.3557 секунд до 0.3316 - поменяем это в тестах
Хорошо, но не так здорово как бы хотелось - продолжаю искать дальше.

### Находка №3
Для дальнейшей оптимизации пришлось разделить метод collect_stats_from_users на много методов и убрать из него вызов блока. Без этого все сливалось в отчете профилировщика. После разделения на много методов стало понятно, что наша беда это цепочки вызовов `map`, которые дают вложенные циклы и поднимают алгоритмическую сложность алгоритма.
После исправления этого имеем следующий результат.

```
Calculating -------------------------------------
                0.1k    560.270  (±28.2%) i/s -      2.481k in   4.995694s
                0.2k    302.473  (±26.1%) i/s -      1.411k in   5.000025s
                0.3k    197.521  (±30.4%) i/s -    840.000  in   4.998514s
                0.4k    137.541  (±26.2%) i/s -    634.000  in   5.000977s
                0.5k    112.263  (±23.2%) i/s -    529.000  in   5.007812s
                 01k     57.508  (±17.4%) i/s -    280.000  in   5.003513s
                 02k     26.251  (±15.2%) i/s -    129.000  in   5.036887s
                 03k     15.526  (±12.9%) i/s -     77.000  in   5.014489s
                 04k     11.816  (±16.9%) i/s -     58.000  in   5.047772s
                 05k      9.592  (±10.4%) i/s -     48.000  in   5.026876s

Comparison:
                0.1k:      560.3 i/s
                0.2k:      302.5 i/s - 1.85x  slower
                0.3k:      197.5 i/s - 2.84x  slower
                0.4k:      137.5 i/s - 4.07x  slower
                0.5k:      112.3 i/s - 4.99x  slower
                 01k:       57.5 i/s - 9.74x  slower
                 02k:       26.3 i/s - 21.34x  slower
                 03k:       15.5 i/s - 36.09x  slower
                 04k:       11.8 i/s - 47.42x  slower
                 05k:        9.6 i/s - 58.41x  slower
```
Время выполнения прогона для файла 10к строк уменьшилось с 0.3316 секунд до 0.1498 - поменяем это в тестах
Попробуем прогнать целевой файл - получилось за 86.1093 секунды!

### Находка №4
При прогоне искомого файла в 100мб программа отжирает 3727 MB, что выглядит подозрительным. Скорее всего дело в закачке файла целиком в память, но что бы быть более уверенным воспользуюсь профилировщиком.
Использование `RubyProf` в режиме `ALLOCATIONS` показали что 48.40% памяти действительно отжирает метод `parse_file` с ним и буду работать в первую очередь.
После замены загрузки файла в память на построчный парсинг потребление памяти сократилось с 3727 MB до 3266 MB. Комитим - смотрим дальше.

### Находка №5
Следующий кандидат на улучшение это `#calc_stat` с 42.70% потребления памяти. Замечаю большую часть из этого потребление памяти берет `Date.parse`. Убираю и получаю сокращение памяти с 3266 MB до 3045 MB и ускорение обработки целевого файла до 61.5225 sec. Стандартный бенчмарк тоже улучшается
```
Calculating -------------------------------------
                0.1k      1.130k (±13.4%) i/s -      5.494k in   4.993676s
                0.2k    634.843  (±12.0%) i/s -      3.111k in   4.996866s
                0.3k    438.975  (±11.6%) i/s -      2.155k in   4.998839s
                0.4k    333.043  (±12.9%) i/s -      1.622k in   4.997612s
                0.5k    272.168  (± 9.2%) i/s -      1.343k in   5.000552s
                 01k    132.632  (± 8.3%) i/s -    657.000  in   4.999300s
                 02k     64.483  (± 7.8%) i/s -    320.000  in   5.004002s
                 03k     38.492  (±13.0%) i/s -    189.000  in   5.008337s
                 04k     28.822  (±10.4%) i/s -    143.000  in   5.012118s
                 05k     22.147  (± 9.0%) i/s -    110.000  in   5.021972s

Comparison:
                0.1k:     1130.3 i/s
                0.2k:      634.8 i/s - 1.78x  slower
                0.3k:      439.0 i/s - 2.57x  slower
                0.4k:      333.0 i/s - 3.39x  slower
                0.5k:      272.2 i/s - 4.15x  slower
                 01k:      132.6 i/s - 8.52x  slower
                 02k:       64.5 i/s - 17.53x  slower
                 03k:       38.5 i/s - 29.36x  slower
                 04k:       28.8 i/s - 39.22x  slower
                 05k:       22.1 i/s - 51.04x  slower
```
Время выполнения прогона для файла 10к строк уменьшилось с 0.1498 секунд до 0.1077 - поменяем это в тестах


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными всего за 61.5 секунды
Удалось улучшить метрику системы (время прогона по файлу 10к строк) с 3.5 секунд до 0.1077 секунд - метрика улучшилась почти в 32 раза!

Следующие кандидаты на оптимизацию, это замена метода `.split` при разборе файла на методы библиотеки CSV, потому что исходный файл именно в этом формате, но на данный момент это можно не делать, потому что время обработки ~60 секунд и потребление памяти ~3Gb вполне приемлемо для запускаемой ночью раз в сутки таски. Дальнейшие работы по улучшению считаю неоправданными с экономической точки зрения.

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы сделан тест на время прохождения контрольного файла в 10к строк. Если время обработки этого файла ухудшится - тест даст нам знать.

## Экономический эффект
Главным экономическим эффектом считаю факт возврата отчета в рабочее состояние. Что бы оценить деньги надо знать какие бизнес-процессы завязаны на этом отчете и сколько каждый из этих процессов терял в день. Дальше сравниваем стоимость потраченного времени разработки с потерями, если бы бизнес процессы работали без этого отчета.

Эффект от сэкономленных 700Мб оперативки считаю незначительным в сравнении с выгодой для бизнеса.
