# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла проблема. Для создания отчета по пользовательским сессиям, было необходимо обработать файл большого размера (> 100 Mb). Существовал черновой вариант обработки файла на `ruby`, который работал при небольшом объеме информации, но не мог быстро (за время, которое бы команда могла дождаться) обработать целевой файл.
Было принято решение об оптимизации программы с использованием специальных инструментов и метрик.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я решил использовать такую метрику: время выполнения программы и объем потребляемой процессом памяти (и allocated objects - memory_profiler)

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за несколько секунд - пару минут в зависимости от файла (некоторые файлы получились слишком большого объема, поэтому их пришлось сократить).

Вот как я построил `feedback_loop`:
- произвел небольшой рефакторинг скрипта (разбил на классы, вынес методы)
- сделал несколько файлов с разным объемом информации (разное число пользователей с их сессиями)
- прогонял скрипт, используя разные инструменты (StackProfiler, MemoryProfiler, Benchmark), вносил изменения, проверял, получилось ли оптимизировать согласно выбранным метрикам.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался StackProfiler, MemoryProfiler, Benchmark.
Следующие проблемы удалось найти и решить:

№1
Использование bang методов вместо обычных для сокращения аллоцированной памяти и числа объектов. Другие небольшие поправки в виде frozen objects, изменения существующих строк, а не создание новых, убрал парсинг дат и т.д.

№2
Обновление версии Ruby до 2.6.1 для использования метода split без инициализации дополнительного массива в памяти при парсинге пользователей и сессий.

user.delete_prefix!(USER_PREFIX).split(COMMA_SEPARATOR) do |atrribute|
  result[USER_ATTRIBUTES[index]] = atrribute
  index += 1
end

№3
Создание спарсеных пользователей и сессий и подсчет их количества во время считывания файла (избавления от времязатратного метода select при поиске связанных сессий)

File.open(file_name) do |f|
  user = nil
  f.each_line do |line|
    if line.start_with?('user')
      users_count += 1
      users << user = User.new(attributes: Parser.parse_user(line))
    else
      parsed_session = Parser.parse_session(line)
      user.sessions << parsed_session
      sessions << parsed_session
      sessions_count += 1
    end
  end
end

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными. Обработать целевой файл удалось за 34 с (самый быстрый прогон).
Удалось улучшить метрику системы для файлов с 4000 пользователями и 10000 пользователями с:
    bench_report	16.701798	101.941843
до:               0.194868	 0.448938

Число аллоцированных объектов также сократилось на сотни тысяч, объем RSS сократился примерно в 2.5 раза (для 10000 пользователей).

RSS для целевого файла колеблется в пределах 2.6-2.9 Gb.

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были добавлены тесты Minitest::Benchmark
