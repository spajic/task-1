# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 20-30% выйгрыш по времени после изменений.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 2.5 минут обработки файла.

Вот как я построил `feedback_loop`:
1. Сократил файл до 50_000 строк
2. Добавил профилеровщик памяти
3. Вносил изменения в исходный код
4. Проверял скорость работы обработки файла
5. Если скорость не менялась, переходил к шагу 3. Если ускорялось, к шагу 6.
6. Увеличивал размер файла в 2 раза и переходил к шагу 3. Если код мог обратывать весь исходный файл за вменяемое время, переходил к шагу 7.
7. Все работает. Задача завершена.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался stackprof и ruby-prof.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
.split(',') занимает достаточно много времени, так как повторялся в методе parse_user и parse_session.
Решение: делать split один раз и уже пробрасывать массив полей в parse_user и parse_session.

### Ваша находка №2
Избыточное использование collect_stats_from_users, где идет сбор статистики по всем пользователям каждый раз для нового аттрибута статистики.
Решение: вынос блока из collect_stats_from_users в отдельные методы.

### Ваша находка №X
Вложенный select в each, используемый для поиска сессий для пользователей, занимал 99% времени.
Решение: рефакторинг и добавление сессий к пользователю на этапе чтений файла.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 2.5 минут для 50_000 записей на 1.5 минут для полного файла(3_250_940 записей)

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавлен новый юнит тест, который зафиксировал 1.5 минуты как эталонное значение. Но с погрешностью в еще 1.5 минуты с учетом загруженность или других факторов на системы, которые могут повлиять на скорость обработки данных.
