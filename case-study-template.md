# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

- Измерять общее время выполнения скрипта с помощью `Benchmark.realtime`
- Использовать профилировщие памяти `memory_profiler`

Для оптимизации был использован тестовый файл 25000 строк (объемом около 1МБ), перед началом оптимизации были следующие показатели:

- Расход памяти: 1655 MB
- Время выполнения: 25.61 sec

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за время меньшее чем 30 секунд

Вот как я построил `feedback_loop`:

- подготовка тестового файла на 25 000 строк
- замеры метрик
- анализ метрик
- исправление самого узкого места
- оценка полученного результата

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался `benchmark` и `memory_profiler`

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Большего всего памяти расходовалось при формировании массивов `users` и `sessions`
Изменил формирование на модификации массивов.
Это позволило снизить расход памяти и немного уменьшить время выполнения.

``` shellsession
rss after concatenation: 1655 MB
Finish in 25.61

rss after concatenation: 491 MB
Finish in 18.09

```

### Ваша находка №2
Следующим узким местом был сам принцип формирования массивов.
Сделал рефакторинг убрав максимально возможное количество временных переменных.
Это позволило сократить время выполения.

``` shellsession
rss after concatenation: 491 MB
Finish in 18.09

rss after concatenation: 418 MB
Finish in 4.57

```

### Ваша находка №3
Следующее узкое место формирование статистики юзера.
Объединил формирование в статистики в один цикл.
Уменьшилось количество потребляемой памяти и время выполнения

``` shellsession
rss after concatenation: 418 MB
Finish in 4.57

rss after concatenation: 370 MB
Finish in 3.68

```

### Ваша находка №4
По метрикам и по моему имеющемуся опыту следущее узкое место это парсинг дат.
Отрефакторил заменив `Date.parse` на `Date.strptime`
В итоге уменьшилось количество потребляемой памяти и время выполнения

``` shellsession
rss after concatenation: 370 MB
Finish in 3.68

rss after concatenation: 314 MB
Finish in 2.87

```

### Ваша находка №5
По метрикам было видно что чаще всего используются стринги ` ` и `, `.
Вынес их в константы, уменьшив потребление памяти и времени.

``` shellsession
rss after concatenation: 314 MB
Finish in 2.87

rss after concatenation: 308 MB
Finish in 2.67

```

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 25 секунд до 0.26 секунды, что позволило завершить обработку исходного файла.


``` shellsession
Start
rss before concatenation: 24 MB
rss after concatenation: 50 MB
Finish in 0.26

```

Файл `data_large.txt` в итоге стал обрабатываться за время 46.93 секунды

``` shellsession
Start
rss before concatenation: 23 MB
rss after concatenation: 2530 MB
Finish in 46.93

```
Также есть дальнейший путь оптимизации, к примеру замена регулярок на `match`,
изменить формирование хешей с `stringify keys` на `symbolize keys`

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы сохрнял среднее значение минимального времени выполения и потребления памяти.
