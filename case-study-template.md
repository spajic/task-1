# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
- измерение общего времени выполнения с помощью `Benchmark.realtime`
- профилирование программы с помощью `ruby-prof`

Время выполнения исходного кода на тестовом файле 50000 строк(2Мb) составляет: 59-61 секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за время меньшее 1 минуты.

Вот как я построил `feedback_loop`:
- тестовый файл 50000 строк(2Мb), исходное время выполнения 59-61 секунд
- поиск самого узкого места
- улучшение кода
- замеры метрик
- запуск тестов
- анализ результатов

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался библиотеками benchmark, ruby-prof.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Больше всего потребление памяти занимает формирование массивов пользователей и сессий.
Решение, замена конкатенации на модификацию массивов `users` и `sessions` 'in place' с помощью `Array#push`

### Ваша находка №2
Удаляем при парсинге не успользуемые в дальнейшем данные.

### Ваша находка №3
Улучшаем алгоритм работы программы - начинаем хранить сессии в хэше, вместо массивов.

### Ваша находка №4
Удалось добиться небольшого ускорения, собрав всю статистику пользователя за одну итерацию (вместо нескольких для каждого вида статистики).

### Ваша находка №5
Оптимизация регулярок (замена `=~` на `match?`, вынос регулярного выражения вне цикла)

### Ваша находка №6
Чтение файла построчно позволяет сэкономить немного памяти и ускорить работу.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы:

Для тестового файла 50000 строк(2Мb), исходное время выполнения 59-61 секунд, до 0,54 секунд
```
Start
rss before: 17 MB
rss after: 61 MB
Finish in 0.54 s
```
Для основного файла до 55 секунд.
```
Start
rss before: 16 MB
rss after: 2231 MB
Finish in 55.53 s
```

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавлен regress тест.
