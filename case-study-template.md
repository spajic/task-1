# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста двадцати мегабайт (3 млн строк).

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решила исправить эту проблему, оптимизировав эту программу.

## Формирование метрики


Сначала я хотела протестировать програму с болсшим файлом, но после 10 минут ожидания
ее окончания, а не смогла дождаться и решила делать оптимизацию с файлом меньшего размера.

Чтобы найти более-менее нормалное колицчество строк, обработка которич не занимает сильно много времени,
была использована метрика с Benchmark.realtime
Я сократила файл до 98_000 строк, но ето не сильно помогло мне, так как обработка файла все равно прочодила
цлишком долго для меня.
В итоге, я сократила количество строк до 30_450, что было эквивалентно 1.1 МБ. - здесь
время обработки файла заняло 50.04 секунды. Эта метрика стала исчодной и основной тоцчкой для меня.

Для начала, я решила использовать все метрики из лекции, чтобы практиковать их применение.
В процессе оптимизации я формировала метрики с помощью:
1. Memory Gem set
2. MemoryProfiler
3.Ruby Prof
4. StackProf

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом.
Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.
Позже тест был переписан в формате RSpec

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроила эффективный `feedback-loop`,
который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построила `feedback_loop`: *как вы построили feedback_loop*
1. Сделать изменение в коде
2. Проверить проходит ли данное изменение в коде
3. Если тест проходит, проверить улуцчшилисж ли показатели по метрикам
4. Если тест не прочодит, цм пункт 1.
5. Если показатели по метрикам приемлемы, push to GitHub

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
1. Benchmark.realtime
2. MemoryProfiler
3. Ruby Prof


Вот какие проблемы удалось найти и решить

### Ваша находка №1
О вашей находке №1
Трудно удержаться от рефакторинга кода сразу

### Ваша находка №2
Несколько проблемных областей были найдены для пользователей `users.each`  и  `sessions.each`
Как показала практика, лучше избегать использования знака конкатенации в `users_objects = users_objects + [user_object]`  и использовать вместо него `<<`.  Таким образом, мы можем избежать создания дополнительных объектов в памяти. Я также старалась избегать присвоения значений дополнительным переменным, когда это было возможно, чтобы чтение программы все еще имело смысл.

Cначала я решил разобраться с кодом для итерации пользователей в строке 128.
 Я также изменилa код для итераций сессий с`uniqueBrowsers += [browser]` на `uniqueBrowsers << session['browser']`.  Таким образом, мы могли бы уменьшить количество дополнительных объектов, созданных ранее.
После изменения кода для использования `<<` и удаления дополнительных переменных мне удалось сократить время обработки файла размером 1 МБ с 50 до 38,44 с.
### Ваша находка 3
Я также заметила с помощью MemoryProfiler, что в этой строке кода было создано много объектов.

```
report['allBrowsers'] =
  sessions
    .map { |s| s['browser'] }
    .map { |b| b.upcase }
    .sort
    .uniq
    .join(',')
```     
Многие изменения должны быть сделаны непосредственно во время чтения файла, особенно создание массива сессий и уникальных браузеров. Это позволило бы нам уменьшить использование `map`, который создает новые объекты массива. Я также использовала `upcase!` Вместо `upcase` для изменения значения на прямую. После рефакторинга кода `report ['allBrowser'] выглядел так:

```
report['allBrowsers'] =
unique_browsers
  .sort
  .join(',')
```

Изменение сессий на хеш, где key - это id пользователя, а value - массив сессий для этого пользователя, который помог избавитьця от метода «select», который неожиданно использовал кучу ресурсов памяти.
Теперь вместо `user_sessions = sessions.select {| session | session ['user_id'] == user ['id']} `мы только что использовали` user_sessions = session [user ['id']] `

После этой оптимизации программа для разбора файла 1MB запустилась за 0,68 секунды.

### Ваша находка 4
`sort_by!` и `reverse!`,  работали лучше, чем `sort` и` reverse` при создании `session dates` После этого оптимизация программы была завершена за 0,48 секунды.

### Ваша находка 5
Нет большой разницы в интерполяции строк между:
`user.sessions.map {| s | s [: time] .to_i} .sum.to_s << 'min.'`
а также
`" # {session_time.sum} min. "`
Также хорошо уменьшить дублирование кода, которое использует `map`

### Ваша находка 6
Лучше использовать символы для ключей хеша вместо строк, потому что ключи-строки будут выделять отдельные объекты при каждом их использовании.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 
```
Finish in 50.6

rss before iteration: 75 MB
rss after iteration: 99 MB
```
 до
```

Finish in 0.38

rss before iteration: 13 MB
rss after iteration: 13 MB
```

## Защита от регресса производительности
We should check tha correctness of tests
We should check that our new metrics do not become worse
